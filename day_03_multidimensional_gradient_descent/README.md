# ğŸ“˜ LESSON 3: MULTIDIMENSIONAL GRADIENT DESCENT

## 1. Theory: How Gradient Descent Generalizes to Multidimensional Cases

### ğŸ”¹ One-Dimensional Gradient Descent Review

In the one-dimensional case, we have a loss function `L(w)` and one parameter `w`.

We calculate the derivative `dL/dw` to understand which direction to move to minimize loss.

**Update formula:**

```
w(t+1) = w(t) - Î· Ã— dL/dw(t)
```

where `Î·` is the learning rate.

### ğŸ”¹ Generalization to Multidimensional Case

Now we have multiple parameters `w = [wâ‚€, wâ‚, ..., wâ‚™]áµ€`.

Loss is a function of many variables: `L(w)`.

To understand how to move in parameter space, we use the **gradient**:

```
âˆ‡w L(w) = [âˆ‚L/âˆ‚wâ‚€, âˆ‚L/âˆ‚wâ‚, ..., âˆ‚L/âˆ‚wâ‚™]áµ€
```

**Parameter update formula becomes vectorized:**

```
w(t+1) = w(t) - Î· Ã— âˆ‡w L(w(t))
```

ğŸ“Œ **Breaking down the elements:**

| Symbol       | Meaning                                                                               |
| ------------ | ------------------------------------------------------------------------------------- |
| `w`          | Vector of model parameters                                                            |
| `Î·`          | Learning rate - speed of movement along gradient direction                            |
| `âˆ‡w L(w(t))` | Gradient: vector of all partial derivatives, points toward steepest function increase |

### âš¡ Geometric Interpretation:

- Gradient shows where function `L(w)` grows fastest
- We go in the opposite direction to decrease loss
- In multidimensional case, gradient is an arrow in n-dimensional space pointing "where the mountain climbs steepest"

### âœ… Quick Understanding Check:

If the gradient equals zero, what does this mean for the loss function?

---

## 2. Analogy for Understanding

### ğŸ’¡ "Walking in Mountains with Blindfolded Eyes"

- You can only feel the steepness at your current location (gradient)
- To go downhill â†’ walk in the opposite direction
- When there are multiple axes (X, Y, Z...) â†’ gradient becomes a vector showing how to move simultaneously in all directions
- Learning rate `Î·` is your "step size" at each movement

### âœ… Understanding Check:

How will your path change if the step size is too large? If too small?

---

## 3. Practice: Gradient Descent for Linear Regression

### Linear regression with two parameters:

```
Å· = wâ‚x + wâ‚€
```

### Loss Function (MSE):

```
L = (1/N) Ã— Î£(i=1 to N) (yáµ¢ - Å·áµ¢)Â²
```

### Gradients:

```
âˆ‚L/âˆ‚wâ‚ = -(2/N) Ã— Î£(i=1 to N) xáµ¢(yáµ¢ - Å·áµ¢)
âˆ‚L/âˆ‚wâ‚€ = -(2/N) Ã— Î£(i=1 to N) (yáµ¢ - Å·áµ¢)
```

### Parameter Updates:

```
wâ‚ â† wâ‚ - Î· Ã— âˆ‚L/âˆ‚wâ‚
wâ‚€ â† wâ‚€ - Î· Ã— âˆ‚L/âˆ‚wâ‚€
```

---

## 4. Python Implementation

```python
import numpy as np

# Data
x = np.array([1, 2, 3, 4])
y = np.array([2, 3, 5, 7])

# Parameters
w0, w1 = 0.0, 0.0
eta = 0.01
epochs = 1000
N = len(x)

# Gradient descent
for epoch in range(epochs):
    y_pred = w1 * x + w0
    dw1 = (-2/N) * np.sum(x * (y - y_pred))
    dw0 = (-2/N) * np.sum(y - y_pred)
    w1 -= eta * dw1
    w0 -= eta * dw0

    if epoch % 200 == 0:
        loss = np.mean((y - y_pred)**2)
        print(f"Epoch {epoch}: Loss = {loss:.4f}, w0 = {w0:.4f}, w1 = {w1:.4f}")

print(f"Final parameters: w0 = {w0:.4f}, w1 = {w1:.4f}")
```

âš¡ **Key Insight:**

- With each step, parameters `wâ‚€, wâ‚` approach optimal values
- One gradient step updates all parameters simultaneously

### âœ… Check:

Why is updating all parameters simultaneously more important than updating them one by one?

---

## 5. Manual Calculation Exercise

### Given Data:

```
(xâ‚, yâ‚) = (1, 2), (xâ‚‚, yâ‚‚) = (2, 3)
```

### Initial Parameters:

```
wâ‚€ = 0, wâ‚ = 0, Î· = 0.1
```

### Step 1: Calculate Predictions

```
Å·â‚ = 0, Å·â‚‚ = 0
```

### Step 2: Calculate Gradients

```
dwâ‚ = -(2/2)(1Ã—(2-0) + 2Ã—(3-0)) = -(2+6) = -8
dwâ‚€ = -(2/2)((2-0) + (3-0)) = -(2+3) = -5
```

### Step 3: Update Parameters

```
wâ‚ â† 0 - 0.1Ã—(-8) = 0 + 0.8 = 0.8
wâ‚€ â† 0 - 0.1Ã—(-5) = 0 + 0.5 = 0.5
```

âœ… **Result:** After one gradient step, parameters became `wâ‚€ = 0.5, wâ‚ = 0.8`

---

## 6. Additional Explanations

### ğŸ”¹ Key Concepts:

- **Gradient vector** is a direction pointer for steepest loss increase
- **Simultaneous parameter update** is crucial for correct minimization in multidimensional space
- In neural networks, each weight and bias is a component of vector `w`
- **One gradient step** means moving in the direction opposite to gradient for all weights simultaneously

---

## 7. Understanding Challenge

### ğŸ¤ Your Task:

Explain in your own words:
ğŸ‘‰ What is multidimensional gradient descent?
ğŸ‘‰ How does it differ from one-dimensional gradient descent?
ğŸ‘‰ Why does it work for models with many parameters?

(Use the blindfolded mountain walking analogy ğŸ”ï¸)

---

## Key Takeaways

- **Multidimensional Gradient Descent** optimizes multiple parameters simultaneously
- **Gradient Vector** points toward steepest increase, we move opposite direction
- **Parameter Updates** happen simultaneously for all weights and biases
- **Linear Regression** is a perfect example with two parameters (slope and intercept)
- **Manual Calculation** helps understand the mathematical foundation

_Happy Learning! ğŸš€_
